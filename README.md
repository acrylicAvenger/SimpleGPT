# SimpleGPT

This is the implementation of a simple GPT model which I have made as a part of my personal project for gaining insight into neural networks, NLP and allied fields. I started working on this project out of curiosity and fascination which sparked my interest in Generative Pre-trained Transformers. 
I am still new to this field and this project is a documentation of my learning and enthusiasm to gain knowledge. 
This code is a result of research from a lot of websites, research papers, youtube video of Andrej Kaparthy, OpenAI and pytorch documentation.
All the references have been attached below and I am open to suggestions, feedbacks and positive transformations.  

# External resources
- Chat-GPT paper from Open-AI - https://arxiv.org/abs/2005.14165
- Open-AI blogpost on Chat GPT - https://openai.com/blog/chatgpt
- Attention is all you need - https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
- Youtube - https://www.youtube.com/watch?v=kCc8FmEb1nY
- Git hub - https://github.com/karpathy/ng-video-lecture
- Tiktoken - https://github.com/openai/tiktoken
- pytorch neural networks - https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html
- Dropout a simple way to prevent neural networks from overfitting - https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
- Residual connection - https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55
- Layernorm - https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html
- Attention - https://towardsdatascience.com/attention-and-its-different-forms-7fc3674d14dc
- Residual blocks - https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec
- Feed forward neural networks - https://en.wikipedia.org/wiki/Feedforward_neural_network
